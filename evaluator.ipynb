{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# -- add new imports here --\n",
    "from surprise import model_selection\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from collections import defaultdict\n",
    "from surprise.dataset import Trainset\n",
    "import surprise\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "# -- add new imports here --\n",
    "from models import get_top_n  # Importez la fonction get_top_n depuis votre fichier models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_evaluation_report() missing 4 required positional arguments: 'eval_config', 'sp_ratings', 'precomputed_dict', and 'available_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/thomascollier/Desktop/recommander/evaluator.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#W3sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(evaluation_dict)\u001b[39m.\u001b[39mT\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#W3sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39m'''# Chargez l'ensemble de données de test\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#W3sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mdf_ratings = load_ratings(False)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#W3sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#W3sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m \u001b[39mprint(\"Top-N Recommendations (LOO Method):\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#W3sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39mprint(top_n_loo)'''\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#W3sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m create_evaluation_report()\n",
      "\u001b[0;31mTypeError\u001b[0m: create_evaluation_report() missing 4 required positional arguments: 'eval_config', 'sp_ratings', 'precomputed_dict', and 'available_metrics'"
     ]
    }
   ],
   "source": [
    "def generate_split_predictions(algo, df_ratings, eval_config):\n",
    "    \"\"\"Generate predictions on a random test set specified in eval_config\"\"\"\n",
    "    # Convertir le DataFrame en Dataset de Surprise\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    ratings_dataset = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "    \n",
    "    # Diviser l'ensemble de données en ensembles d'entraînement et de test\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=eval_config.test_size, random_state=42)\n",
    "    \n",
    "    # Entraîner l'algorithme sur l'ensemble d'entraînement\n",
    "    algo.fit(trainset)\n",
    "    \n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    predictions = algo.test(testset)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, df_ratings, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\"\"\"\n",
    "    # Convertir le DataFrame en Dataset de Surprise\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    ratings_dataset = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "    \n",
    "    # Créer un split LeaveOneOut\n",
    "    loo = LeaveOneOut(n_splits=1, random_state=eval_config.random_state)\n",
    "\n",
    "    # Get the train and test sets\n",
    "    for trainset, testset in loo.split(ratings_dataset):\n",
    "        # Entraîner l'algorithme sur le trainset\n",
    "        algo.fit(trainset)\n",
    "        \n",
    "        # Get the anti-testset\n",
    "        anti_testset = trainset.build_anti_testset()\n",
    "        \n",
    "        # Faire des prédictions sur l'anti-testset\n",
    "        all_predictions = algo.test(anti_testset)\n",
    "        \n",
    "        # Initialiser un dictionnaire pour stocker les meilleures recommandations pour chaque utilisateur\n",
    "        top_n_recommendations = defaultdict(list)\n",
    "        \n",
    "        # Sélectionner les meilleures recommandations pour chaque utilisateur\n",
    "        for uid, iid, _, est, _ in all_predictions:\n",
    "            top_n_recommendations[uid].append((iid, est))\n",
    "        \n",
    "        # Trier les recommandations pour chaque utilisateur par note estimée\n",
    "        for uid, user_ratings in top_n_recommendations.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n_recommendations[uid] = user_ratings[:eval_config.top_n_value]\n",
    "\n",
    "    return top_n_recommendations, testset\n",
    "\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, df_ratings, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user with full training set (LOO)\"\"\"\n",
    "    # Convertir le DataFrame en Dataset de Surprise\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    ratings_dataset = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "    \n",
    "    # Construire le trainset complet à partir du Dataset\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "    \n",
    "    # Entraîner l'algorithme sur le trainset complet\n",
    "    algo.fit(full_trainset)\n",
    "    \n",
    "    # Obtenir les anti-testset\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "    \n",
    "    # Faire des prédictions sur l'anti-testset\n",
    "    all_predictions = algo.test(anti_testset)\n",
    "    \n",
    "    # Initialiser un dictionnaire pour stocker les meilleures recommandations pour chaque utilisateur\n",
    "    top_n_recommendations = defaultdict(list)\n",
    "    \n",
    "    # Sélectionner les meilleures recommandations pour chaque utilisateur\n",
    "    for uid, iid, _, est, _ in all_predictions:\n",
    "        top_n_recommendations[uid].append((iid, est))\n",
    "    \n",
    "    # Trier les recommandations pour chaque utilisateur par note estimée\n",
    "    for uid, user_ratings in top_n_recommendations.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n_recommendations[uid] = user_ratings[:eval_config.top_n_value]\n",
    "\n",
    "    return top_n_recommendations\n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\" Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    \n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings\n",
    "    - (-- for your project, add other relevant information here -- )\n",
    "    \"\"\"\n",
    "    precomputed_dict = {}\n",
    "    precomputed_dict[\"item_to_rank\"] = None\n",
    "    return precomputed_dict                \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **precomputed_dict,\n",
    "                    **parameters\n",
    "                )\n",
    "        \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T\n",
    "\n",
    "\n",
    "'''# Chargez l'ensemble de données de test\n",
    "df_ratings = load_ratings(False)\n",
    "\n",
    "# Créez une instance de l'algorithme que vous souhaitez tester\n",
    "algo = surprise.SVD()\n",
    "\n",
    "# Testez la fonction generate_split_predictions\n",
    "predictions_split = generate_split_predictions(algo, df_ratings, EvalConfig)\n",
    "top_n_split = get_top_n(predictions_split, n=EvalConfig.top_n_value)\n",
    "\n",
    "# Testez la fonction generate_loo_top_n\n",
    "top_n_loo, _ = generate_loo_top_n(algo, df_ratings, EvalConfig)\n",
    "\n",
    "# Testez la fonction generate_full_top_n\n",
    "top_n_full = generate_full_top_n(algo, df_ratings, EvalConfig)\n",
    "\n",
    "# Vérifiez les résultats\n",
    "print(\"Top-N Recommendations (Split Method):\")\n",
    "print(top_n_split)\n",
    "\n",
    "print(\"Top-N Recommendations (LOO Method):\")\n",
    "print(top_n_loo)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\n",
    "    \"\"\"\n",
    "    # -- implement the function get_hit_rate --\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    \"\"\"Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    The novelty is defined as the average ranking of the movies recommended\n",
    "    \"\"\"\n",
    "    # -- implement the function get_novelty --\n",
    "    return average_rank_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/thomascollier/Desktop/recommander/evaluator.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Dans le fichier configs.py\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m AVAILABLE_METRICS \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m\"\u001b[39m: (accuracy\u001b[39m.\u001b[39mmae, {\u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m}),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Ajoutez de nouveaux types de métriques ici\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m sp_ratings \u001b[39m=\u001b[39m load_ratings(surprise_format\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m precomputed_dict \u001b[39m=\u001b[39m precompute_information()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thomascollier/Desktop/recommander/evaluator.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m evaluation_report \u001b[39m=\u001b[39m create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
      "File \u001b[0;32m~/Desktop/recommander/loaders.py:22\u001b[0m, in \u001b[0;36mload_ratings\u001b[0;34m(surprise_format)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m surprise_format:\n\u001b[1;32m     20\u001b[0m     \u001b[39m# Convertir le DataFrame en Dataset surprise\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     reader \u001b[39m=\u001b[39m Reader(rating_scale\u001b[39m=\u001b[39mC\u001b[39m.\u001b[39mRATINGS_SCALE)\n\u001b[0;32m---> 22\u001b[0m     surprise_dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mload_from_df(df_ratings, (reader))\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m surprise_dataset\n\u001b[1;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/surprise/dataset.py:167\u001b[0m, in \u001b[0;36mDataset.load_from_df\u001b[0;34m(cls, df, reader)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_df\u001b[39m(\u001b[39mcls\u001b[39m, df, reader):\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a dataset from a pandas dataframe.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m    Use this if you want to use a custom dataset that is stored in a pandas\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m            specified.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m DatasetAutoFolds(reader\u001b[39m=\u001b[39mreader, df\u001b[39m=\u001b[39mdf)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/surprise/dataset.py:264\u001b[0m, in \u001b[0;36mDatasetAutoFolds.__init__\u001b[0;34m(self, ratings_file, reader, df)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39melif\u001b[39;00m df \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m df\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_ratings \u001b[39m=\u001b[39m [\n\u001b[1;32m    263\u001b[0m         (uid, iid, \u001b[39mfloat\u001b[39m(r), \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 264\u001b[0m         \u001b[39mfor\u001b[39;00m (uid, iid, r) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mitertuples(index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    265\u001b[0m     ]\n\u001b[1;32m    266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMust specify ratings file or dataframe.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Dans le fichier configs.py\n",
    "\n",
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "        # Ajoutez de nouvelles métriques de division ici\n",
    "    },\n",
    "    # Ajoutez de nouveaux types de métriques ici\n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "precomputed_dict = precompute_information()\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "export_evaluation_report(evaluation_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
