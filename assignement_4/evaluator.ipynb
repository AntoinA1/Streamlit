{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -- add new imports here --\n",
    "from surprise import model_selection\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import LeaveOneOut\n",
    "from collections import defaultdict\n",
    "from surprise.dataset import Trainset\n",
    "import surprise\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_items\n",
    "from loaders import load_ratings\n",
    "\n",
    "# -- add new imports here --\n",
    "from models import get_top_n  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/small/evidence/ratings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Streamlit/assignement_4/evaluator.ipynb Cellule 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bmusical-train-jj57qw7qrx9jh5xq7/workspaces/Streamlit/assignement_4/evaluator.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(evaluation_dict)\u001b[39m.\u001b[39mT\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bmusical-train-jj57qw7qrx9jh5xq7/workspaces/Streamlit/assignement_4/evaluator.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39m# Load data \u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://codespaces%2Bmusical-train-jj57qw7qrx9jh5xq7/workspaces/Streamlit/assignement_4/evaluator.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m df_ratings \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(C\u001b[39m.\u001b[39;49mEVIDENCE_PATH \u001b[39m/\u001b[39;49m C\u001b[39m.\u001b[39;49mRATINGS_FILENAME, usecols\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39muserId\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmovieId\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrating\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bmusical-train-jj57qw7qrx9jh5xq7/workspaces/Streamlit/assignement_4/evaluator.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39m# Load of the datatest\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bmusical-train-jj57qw7qrx9jh5xq7/workspaces/Streamlit/assignement_4/evaluator.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m df_ratings_test \u001b[39m=\u001b[39m load_ratings(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/small/evidence/ratings.csv'"
     ]
    }
   ],
   "source": [
    "def generate_split_predictions(algo, df_ratings, eval_config):\n",
    "    \"\"\"Generate predictions on a random test set specified in eval_config\"\"\"\n",
    "\n",
    "    # Get the surprise dataset\n",
    "    reader = Reader(rating_scale=(0.5,5.0))\n",
    "    ratings_dataset = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "   \n",
    "    # Split the dataset into trainset and testset\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=eval_config.test_size, random_state=42)\n",
    "   \n",
    "    # Train the algorithm on the training set\n",
    "    algo.fit(trainset) \n",
    "   \n",
    "    # Make predictions on the test set\n",
    "    predictions = algo.test(testset)\n",
    "   \n",
    "    return predictions\n",
    " \n",
    " \n",
    "def generate_loo_top_n(algo, df_ratings, eval_config):\n",
    "    \"\"\"Generate top-N recommendations for each user on a random Leave-one-out split (LOO)\"\"\"\n",
    "    # Convert the DataFrame to a Surprise Dataset\n",
    "    reader = Reader(rating_scale=(0.5,5.0))\n",
    "    ratings_dataset = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "   \n",
    "    # Create a split with LeaveOneOut\n",
    "    loo = LeaveOneOut(n_splits=1, random_state=eval_config.random_state)\n",
    " \n",
    "    # Get the trainset and testset\n",
    "    for trainset, testset in loo.split(ratings_dataset):\n",
    "        # Train the algorithm on the trainset\n",
    "        algo.fit(trainset)\n",
    "       \n",
    "        # Get the anti-testset\n",
    "        anti_testset = trainset.build_anti_testset()\n",
    "       \n",
    "        # Make predictions on the anti-testset\n",
    "        all_predictions = algo.test(anti_testset)\n",
    "       \n",
    "        # Initialize a dictionary to store the top-N recommendations for each user\n",
    "        top_n_recommendations = defaultdict(list)\n",
    "       \n",
    "        # Select the top-N recommendations for each user\n",
    "        for uid, iid, _, est, _ in all_predictions:\n",
    "            top_n_recommendations[uid].append((iid, est))\n",
    "       \n",
    "        # Sort the recommendations for each user by estimated rating\n",
    "        for uid, user_ratings in top_n_recommendations.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n_recommendations[uid] = user_ratings[:eval_config.top_n_value]\n",
    " \n",
    "    return top_n_recommendations, testset\n",
    "\n",
    " \n",
    "def generate_full_top_n(algo, df_ratings, eval_config):\n",
    "    \"\"\"Generate top-N recommendations for each user on the full training set\"\"\"\n",
    "\n",
    "    # Convert the DataFrame to a Surprise Dataset\n",
    "    reader = Reader(rating_scale=(0.5,5.0))\n",
    "    ratings_dataset = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "   \n",
    "    # Build the full training set\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "   \n",
    "    # Train the algorithm on the full training set\n",
    "    algo.fit(full_trainset)\n",
    "   \n",
    "    # Generate anti-testset recommendations\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "   \n",
    "    # Make predictions on the anti-testset\n",
    "    all_predictions = algo.test(anti_testset)\n",
    "   \n",
    "    # Initialize a dictionary to store the top-N recommendations for each user\n",
    "    top_n_recommendations = defaultdict(list)\n",
    "   \n",
    "    # Select the top-N recommendations for each user\n",
    "    for uid, iid, _, est, _ in all_predictions:\n",
    "        top_n_recommendations[uid].append((iid, est))\n",
    "   \n",
    "    # Sort the recommendations for each user by estimated rating\n",
    "    for uid, user_ratings in top_n_recommendations.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n_recommendations[uid] = user_ratings[:eval_config.top_n_value]\n",
    " \n",
    "    return top_n_recommendations\n",
    "\n",
    "def precompute_information(df_items, df_ratings):\n",
    "    \"\"\" Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "   \n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to popularity ranks\n",
    "    - (-- for your project, add other relevant information here -- )\n",
    "    \"\"\"\n",
    "    # Initialize the list\n",
    "    precomputed_dict = {}\n",
    "   \n",
    "    # Calculate the average rating for each movie\n",
    "    avg_rating = df_ratings.groupby('movieId')['rating'].mean().reset_index()\n",
    "   \n",
    "    # Normalize the average ratings to get popularity ranks\n",
    "    avg_rating['popularity_rank'] = avg_rating['rating'].rank(ascending=False)\n",
    "   \n",
    "    # Create a dictionary mapping movie IDs to their popularity ranks\n",
    "    item_to_rank = dict(zip(avg_rating['movieId'], avg_rating['popularity_rank']))\n",
    "    precomputed_dict[\"item_to_rank\"] = item_to_rank\n",
    "   \n",
    "    # You can add other relevant information here\n",
    "    return precomputed_dict\n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "       \n",
    "        # Type 1 : Evaluate split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, df_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters)\n",
    " \n",
    "        # Type 2 : Evaluate loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, df_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters = available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "       \n",
    "        #Type 3 : Evaluate full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, df_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                if metric == 'novelty':\n",
    "                    evaluation_dict[model_name][metric] = get_novelty(anti_testset_top_n, precomputed_dict[\"item_to_rank\"])\n",
    "                else:\n",
    "                    evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                        anti_testset_top_n,\n",
    "                        **parameters\n",
    "    )\n",
    "       \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T\n",
    " \n",
    " \n",
    "# Load data \n",
    "df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME, usecols=['userId', 'movieId', 'rating'])\n",
    " \n",
    "# Load of the datatest\n",
    "df_ratings_test = load_ratings(False)\n",
    " \n",
    "# Initialize an instance of the algorythm \n",
    "algo = surprise.SVD()\n",
    " \n",
    "# Test the function generate_split_predictions\n",
    "predictions_split = generate_split_predictions(algo, df_ratings_test, EvalConfig)\n",
    "top_n_split = get_top_n(predictions_split, n=EvalConfig.top_n_value)\n",
    " \n",
    "# Test the function generate_loo_top_n\n",
    "top_n_loo, _ = generate_loo_top_n(algo, df_ratings_test, EvalConfig)\n",
    " \n",
    "# Test the function generate_full_top_n\n",
    "top_n_full = generate_full_top_n(algo, df_ratings_test, EvalConfig)\n",
    " \n",
    "# Print the results\n",
    "print(\"Split predictions:\")\n",
    "print(top_n_split)\n",
    " \n",
    "print(\"Leave-One-Out predictions:\")\n",
    "print(top_n_loo)\n",
    " \n",
    "print(\"Full predictions:\")\n",
    "print(top_n_full)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Calculate the hit rate metric.\"\"\"\n",
    "\n",
    "    # Intialize variables\n",
    "    total_users = len(testset)\n",
    "    total_hits = 0\n",
    "   \n",
    "    for user_id, movie_id, _ in testset:\n",
    "        \n",
    "        # Verify if the evaluated movie by user is in the top-N recommandations \n",
    "        if user_id in anti_testset_top_n and movie_id in [movie_id for movie_id, _ in anti_testset_top_n[user_id]]:\n",
    "            total_hits += 1\n",
    "   \n",
    "    # Calculate the hit rate\n",
    "    hit_rate = total_hits / total_users if total_users > 0 else 0.0\n",
    "    return hit_rate\n",
    " \n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    \"\"\"Calculate the average novelty of the recommended top-n lists.\"\"\"\n",
    "    total_novelty = 0\n",
    "    total_users = len(anti_testset_top_n)\n",
    "   \n",
    "    for user_id, top_n_recommendations in anti_testset_top_n.items():\n",
    "        user_novelty = sum(item_to_rank.get(item_id, 0) for item_id, _ in top_n_recommendations)\n",
    "        total_novelty += user_novelty\n",
    "   \n",
    "    return total_novelty / total_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling model contentBased-random_forest-year\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n",
      "Handling model contentBased-lasso_regression-year\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n",
      "Handling model contentBased-ridge_regression-year\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n",
      "Handling model contentBased-linear_regression-year\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n",
      "Handling model contentBased-gradient_boosting-year\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n",
      "                                          mae      rmse  hit_rate  \\\n",
      "contentBased-random_forest-year      0.862402  1.125898  0.056075   \n",
      "contentBased-lasso_regression-year   0.777163  0.989062  0.028037   \n",
      "contentBased-ridge_regression-year   0.778132  0.990388  0.028037   \n",
      "contentBased-linear_regression-year  0.778143  0.990406  0.028037   \n",
      "contentBased-gradient_boosting-year  0.884208  1.163197  0.056075   \n",
      "\n",
      "                                          novelty  \n",
      "contentBased-random_forest-year      14373.579439  \n",
      "contentBased-lasso_regression-year   14556.953271  \n",
      "contentBased-ridge_regression-year   14551.985981  \n",
      "contentBased-linear_regression-year  14551.985981  \n",
      "contentBased-gradient_boosting-year  13934.331776  \n"
     ]
    }
   ],
   "source": [
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False}),\n",
    "        \n",
    "    },\n",
    "    \"loo\": {\n",
    "\n",
    "        # Add hit metric\n",
    "        \"hit_rate\": (calculate_hit_rate, {}),\n",
    "    },\n",
    "    \"full\": {\n",
    "\n",
    "        # Add get novelty\n",
    "        \"novelty\": (get_novelty, {}),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load data\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "df_items = load_items()\n",
    "\n",
    "# Precalculate needed information\n",
    "precomputed_dict = precompute_information(df_items, df_ratings)\n",
    " \n",
    "# Make the report of evaluation\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "print(evaluation_report)\n",
    "\n",
    "# Export the report in a file \n",
    "export_evaluation_report(evaluation_report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
