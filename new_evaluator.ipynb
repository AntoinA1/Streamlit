{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise.model_selection import train_test_split, LeaveOneOut\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from sklearn.metrics import jaccard_score\n",
    "import itertools\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import load_ratings, export_evaluation_report\n",
    "from models import get_top_n, ContentBased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(surprise_format=False):\n",
    "    df_ratings = pd.read_csv(C.EVIDENCE_PATH / C.RATINGS_FILENAME)\n",
    "    if surprise_format:\n",
    "        reader = Reader(rating_scale=C.RATINGS_SCALE)\n",
    "        data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "        return data\n",
    "    else:\n",
    "        return df_ratings\n",
    "\n",
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate predictions on a random test set specified in eval_config\"\"\"\n",
    "    # Split the data into training and testing sets based on the proportion defined in eval_config\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=eval_config.test_size)\n",
    "\n",
    "    # Train the recommendation algorithm on the training set\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    # Generate predictions on the test set\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\"\"\"\n",
    "    loo = LeaveOneOut()\n",
    "    top_n_recommendations = None\n",
    "    testset = None\n",
    "\n",
    "    for trainset, testset in loo.split(ratings_dataset):\n",
    "        # Fit the algorithm on the trainset\n",
    "        algo.fit(trainset)\n",
    "\n",
    "        # Test the algorithm on the single excluded rating\n",
    "        predictions = algo.test(testset)\n",
    "\n",
    "        # Build the anti-testset for all users in the training set\n",
    "        anti_testset = trainset.build_anti_testset()\n",
    "\n",
    "        # Test the algorithm on the anti-testset\n",
    "        all_predictions = algo.test(anti_testset)\n",
    "\n",
    "        # Get the top-N recommendations from the predictions\n",
    "        top_n_recommendations = get_top_n(all_predictions, n=eval_config.top_n_value)\n",
    "        \n",
    "        # Break after the first split, since LOO method is usually executed one split at a time for evaluation\n",
    "        break\n",
    "\n",
    "    return top_n_recommendations, testset\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user with full training set (LOO)\"\"\"\n",
    "    # Build the full training set from the dataset\n",
    "    full_trainset = ratings_dataset.build_full_trainset()\n",
    "\n",
    "    # Fit the algorithm on the full training set\n",
    "    algo.fit(full_trainset)\n",
    "\n",
    "    # Build the anti-testset from the full training set\n",
    "    anti_testset = full_trainset.build_anti_testset()\n",
    "\n",
    "    # Test the algorithm on the anti-testset\n",
    "    all_predictions = algo.test(anti_testset)\n",
    "\n",
    "    # Get the top-N recommendations from the predictions\n",
    "    anti_testset_top_n = get_top_n(all_predictions, n=eval_config.top_n_value)\n",
    "\n",
    "    return anti_testset_top_n\n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\" Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    \n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings\n",
    "    - (-- for your project, add other relevant information here -- )\n",
    "    \"\"\"\n",
    "    # Count the number of ratings by movieId and sort in descending order of popularity\n",
    "    ratings = load_ratings()\n",
    "    item_counts = ratings['movieId'].value_counts().sort_values(ascending=False)\n",
    "    item_to_rank = {movie: idx + 1 for idx, movie in enumerate(item_counts.index)}\n",
    "    \n",
    "    return {'item_to_rank': item_to_rank}\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters = available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters = available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters = available_metrics[\"full\"][metric]\n",
    "                if metric == 'diversity':\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n)\n",
    "                elif metric == 'precision':\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset)\n",
    "                else:\n",
    "                    evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                        anti_testset_top_n,\n",
    "                        **precomputed_dict,\n",
    "                        **parameters\n",
    "                    )\n",
    "        \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Compute the average hit over the users (loo metric)\n",
    "    \n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    total = len(testset)  \n",
    "\n",
    "    for user_id, movie_id, _ in testset:\n",
    "        top_n_recommendations = anti_testset_top_n.get(user_id, [])\n",
    "        if movie_id in [recommended_movie[0] for recommended_movie in top_n_recommendations]:\n",
    "            hits += 1 \n",
    "\n",
    "    hit_rate = hits / total if total > 0 else 0\n",
    "\n",
    "    return hit_rate\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    total_rank = 0\n",
    "    num_entries = 0\n",
    "    max_rank = len(item_to_rank)\n",
    "    for user_recommendations in anti_testset_top_n.values():\n",
    "        for movie_id, _ in user_recommendations:\n",
    "            total_rank += item_to_rank.get(movie_id, max_rank + 1)\n",
    "            num_entries += 1\n",
    "    average_rank_sum = total_rank / num_entries if num_entries > 0 else 0\n",
    "    normalized_novelty = average_rank_sum / max_rank  # Normalization step\n",
    "    return normalized_novelty\n",
    "\n",
    "def calculate_diversity(top_n_recommendations):\n",
    "    all_recommendations = list(top_n_recommendations.values())\n",
    "    total_pairs = 0\n",
    "    total_diversity = 0\n",
    "    for list1, list2 in itertools.combinations(all_recommendations, 2):\n",
    "        items1 = set([iid for iid, _ in list1])\n",
    "        items2 = set([iid for iid, _ in list2])\n",
    "        jaccard_dist = 1 - len(items1 & items2) / len(items1 | items2)\n",
    "        total_diversity += jaccard_dist\n",
    "        total_pairs += 1\n",
    "    return total_diversity / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "def calculate_precision(top_n_recommendations, testset):\n",
    "    total_relevant = 0\n",
    "    total_recommended = 0\n",
    "    for uid, user_ratings in top_n_recommendations.items():\n",
    "        relevant_items = {iid for (uid, iid, true_r) in testset if true_r >= 4}\n",
    "        recommended_items = {iid for (iid, est) in user_ratings}\n",
    "        total_relevant += len(recommended_items & relevant_items)\n",
    "        total_recommended += len(recommended_items)\n",
    "    return total_relevant / total_recommended if total_recommended > 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        \"rmse\": (accuracy.rmse, {'verbose': False})\n",
    "    },\n",
    "    \"loo\": {\n",
    "        \"hit_rate\": (get_hit_rate, {})\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"novelty\": (get_novelty, {}),\n",
    "        \"diversity\": (calculate_diversity, {}),\n",
    "        \"precision\": (calculate_precision, {})\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to evaluate ContentBased class with different feature methods and regression methods\n",
    "def evaluate_content_based():\n",
    "    feature_methods = [\n",
    "        'year_of_release', 'average_rating', 'rating_count', 'genre_tf_idf',\n",
    "          'tag', 'synopsis', 'previous_apparitions', 'actors_tfidf',\n",
    "        'director_tfidf', 'production_countries_tfidf', 'budget',\n",
    "        'original_language_tfidf', 'production_companies_tfidf', 'translations_tfidf'\n",
    "    ]\n",
    "    \n",
    "    regressor_methods = [\n",
    "        'linear', 'gradient_boosting', 'neural_network',\n",
    "        'decision_tree','random_forest'\n",
    "    ]\n",
    "    \n",
    "    results = {metric: [] for metric in AVAILABLE_METRICS['split'].keys()}\n",
    "    results.update({metric: [] for metric in AVAILABLE_METRICS['loo'].keys()})\n",
    "    results.update({metric: [] for metric in AVAILABLE_METRICS['full'].keys()})\n",
    "    \n",
    "    for regressor in regressor_methods:\n",
    "        for feature in feature_methods:\n",
    "            print(f\"Evaluating: Regressor: {regressor}, Feature: {feature}\")\n",
    "            config = EvalConfig()\n",
    "            config.models = [('ContentBased', ContentBased, {'features_method': [feature], 'regressor_method': regressor})]\n",
    "            sp_ratings = load_ratings(surprise_format=True)\n",
    "            precomputed_dict = precompute_information()\n",
    "            evaluation_report = create_evaluation_report(config, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "            for metric in results.keys():\n",
    "                results[metric].append({\n",
    "                    'regressor': regressor,\n",
    "                    'feature': feature,\n",
    "                    metric: evaluation_report[metric].loc['ContentBased']\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_content_based()\n",
    "\n",
    "# Convert results to DataFrame and save to CSV for each metric\n",
    "for metric, result in results.items():\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(f'content_based_evaluation_{metric}.csv', index=False)\n",
    "    print(f\"Saved results for {metric} to content_based_evaluation_{metric}.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
